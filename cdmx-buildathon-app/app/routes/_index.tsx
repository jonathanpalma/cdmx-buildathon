import type { Route } from "./+types/_index";
import { useState, useCallback, useMemo } from "react"
import { AudioUpload } from "~/components/audio/audio-upload"
import { AudioPlaybackSimulator } from "~/components/audio/audio-playback-simulator"
import { LiveTranscriptImproved as LiveTranscript, type TranscriptEntry } from "~/components/audio/live-transcript-improved"
import { AgentCopilot, type CurrentStepData } from "~/components/copilot"
import type { AgentState, ConversationStage } from "~/lib/agent/state"

export function meta({}: Route.MetaArgs) {
  return [
    { title: "CloseLoop - AI Call Center Copilot" },
    { name: "description", content: "AI-powered copilot for call center agents" },
  ];
}

// Helper functions for stage data
function getStageDescription(stageIndex: number): string {
  const descriptions = [
    "Establish rapport and identify customer needs",
    "Gather travel details and preferences",
    "Match customer needs to ideal property",
    "Present value and close the booking",
    "Confirm booking and set expectations",
  ]
  return descriptions[stageIndex] || descriptions[0]
}

function getTipsForStage(stageIndex: number): string[] {
  const tips: Record<number, string[]> = {
    0: [
      "Smile while speaking - it changes your tone",
      "Use customer's name if you have it",
      "Listen for emotional cues (excitement, hesitation)",
    ],
    1: [
      "Ask about special occasions (anniversary, birthday)",
      "Note dietary restrictions or accessibility needs",
      "Capture budget range without being pushy",
    ],
    2: [
      "Use words like 'perfect for your family' to personalize",
      "Mention 2-3 key amenities that match their needs",
      "Create urgency with limited availability or expiring promos",
    ],
    3: [
      "Frame as 'per person per night' to make it feel affordable",
      "Compare to alternative vacation costs (airfare + hotel + food)",
      "Ask if they want to secure the reservation",
    ],
    4: [
      "Thank them for choosing Palace Resorts",
      "Mention post-booking concierge services",
      "Ask for referrals if conversation went well",
    ],
  }
  return tips[stageIndex] || []
}

export default function Index() {
  const [audioFile, setAudioFile] = useState<File | null>(null)
  const [transcriptEntries, setTranscriptEntries] = useState<TranscriptEntry[]>([])
  const [currentTime, setCurrentTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [conversationHealth, setConversationHealth] = useState(75)

  // AI Agent state (replaces static currentStepData)
  const [agentState, setAgentState] = useState<Partial<AgentState>>({})

  // Agent call management (debouncing + cancellation)
  const abortControllerRef = useRef<AbortController | null>(null)
  const debounceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const maxWaitTimerRef = useRef<NodeJS.Timeout | null>(null)
  const pendingMessagesRef = useRef<TranscriptEntry[]>([])

  // Conversation stages come directly from agent (dynamically generated)
  const stages = useMemo<ConversationStage[]>(() => {
    return agentState.conversationStages || []
  }, [agentState.conversationStages])

  // Current step data - use AI agent state if available, otherwise show empty state
  const currentStepData = useMemo<CurrentStepData | null>(() => {
    // Don't show anything until we have transcript entries
    if (transcriptEntries.length === 0) {
      return null
    }

    // If agent has generated suggestions, use them
    if (agentState.nextActions && agentState.nextActions.length > 0) {
      const currentStage = stages.find(s => s.status === "current")

      return {
        stage: agentState.currentStage || currentStage?.label || "In Progress",
        description: currentStage?.description || "AI is analyzing the conversation...",
        aiSuggestion: agentState.reasoning || undefined,
        nextActions: agentState.nextActions,
        script: agentState.nextActions[0]?.description,
        tips: [], // Tips can be generated by agent if needed
      }
    }

    // If no agent data yet, return null (shows waiting state in copilot)
    return null
  }, [agentState, stages, transcriptEntries.length])

  // Handle file selection
  const handleFileSelect = useCallback((file: File) => {
    setAudioFile(file)
    setTranscriptEntries([])
    setIsPlaying(false)
    setConversationHealth(75)
    setAgentState({}) // Reset agent state
  }, [])

  // Call AI Agent to analyze conversation and generate suggestions
  const callAgent = useCallback(async (newEntry: TranscriptEntry) => {
    try {
      const response = await fetch("/api/agent", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          message: {
            speaker: newEntry.speaker,
            text: newEntry.text,
            timestamp: newEntry.timestamp,
          },
          currentState: agentState,
        }),
      })

      if (response.ok) {
        const { state } = await response.json()
        setAgentState(state)

        // Update UI with agent's analysis
        setConversationHealth(state.healthScore || 75)
      }
    } catch (error) {
      console.error("Agent call failed:", error)
    }
  }, [agentState])

  // Handle transcript updates from playback
  const handleTranscriptUpdate = useCallback((event: any) => {
    setCurrentTime(event.timestamp)

    // Skip empty transcriptions (silence or API returned no text)
    if (!event.text || !event.text.trim() || event.isEmpty) {
      return
    }

    setTranscriptEntries((prev) => {
      // Check if this message already exists
      const exists = prev.some(
        (entry) =>
          entry.timestamp === event.timestamp && entry.text === event.text
      )

      if (exists) {
        return prev
      }

      // Check if we should merge with the last entry from the same speaker
      const lastEntry = prev[prev.length - 1]
      const shouldMerge =
        lastEntry &&
        lastEntry.speaker === event.speaker &&
        event.timestamp - lastEntry.timestamp < 5 // Merge if within 5 seconds

      if (shouldMerge) {
        // Create merged entry
        const mergedEntry: TranscriptEntry = {
          ...lastEntry,
          text: `${lastEntry.text} ${event.text}`,
          confidence: (lastEntry.confidence! + event.confidence) / 2, // Average confidence
        }

        // Call agent with merged message
        callAgent(mergedEntry)

        // Replace last entry with merged one
        return [...prev.slice(0, -1), mergedEntry]
      }

      // Create new entry
      const newEntry: TranscriptEntry = {
        id: `entry-${event.chunkIndex}-${event.timestamp}`,
        timestamp: event.timestamp,
        text: event.text,
        speaker: event.speaker,
        confidence: event.confidence,
        isFinal: true,
      }

      const newEntries = [...prev, newEntry]

      // Call AI agent to analyze the new message
      callAgent(newEntry)

      return newEntries
    })
  }, [callAgent])

  const handleActionClick = useCallback((actionId: string) => {
    // TODO: Implement action handling logic
    // This will trigger specific actions like checking availability, generating quotes, etc.
    console.log("Action clicked:", actionId)
  }, [])

  const handleFeedback = useCallback((actionId: string, positive: boolean) => {
    // TODO: Track agent feedback for ML improvement
    console.log("Feedback:", actionId, positive ? "üëç" : "üëé")
  }, [])

  return (
    <div className="h-screen bg-gray-50 flex flex-col overflow-hidden">
      {/* Header */}
      <div className="bg-white border-b flex-shrink-0">
        <div className="max-w-7xl mx-auto px-4 py-3">
          <div className="flex items-center justify-between">
            <div className="flex items-center gap-3">
              <h1 className="text-xl font-bold text-gray-900">CloseLoop</h1>
              <span className="text-sm text-gray-500">AI Call Center Copilot</span>
            </div>
            {audioFile && (
              <button
                type="button"
                onClick={() => {
                  setAudioFile(null)
                  setTranscriptEntries([])
                  setIsPlaying(false)
                  setConversationHealth(75)
                  setAgentState({})
                }}
                className="px-3 py-1.5 text-sm text-gray-600 hover:text-gray-900 border rounded-lg hover:bg-gray-50"
              >
                Upload New Audio
              </button>
            )}
          </div>
        </div>
      </div>

      <div className="flex-1 min-h-0 overflow-hidden">
        <div className="max-w-7xl mx-auto px-4 py-6 h-full flex flex-col">
        {/* Audio Upload Section */}
        {!audioFile && (
          <div className="flex-1 overflow-auto">
          <div className="max-w-2xl mx-auto">
            <div className="text-center mb-8">
              <h2 className="text-2xl font-semibold text-gray-900 mb-3">
                Upload a Call Recording
              </h2>
              <p className="text-gray-600">
                Upload an audio or video file to see real-time transcription with AI-powered conversation guidance
              </p>
            </div>
            <AudioUpload onFileSelect={handleFileSelect} />

            <div className="mt-8 bg-blue-50 border border-blue-200 rounded-lg p-6">
              <h3 className="font-semibold text-blue-900 mb-3">How it works:</h3>
              <ol className="space-y-2 text-sm text-blue-800">
                <li className="flex gap-2">
                  <span className="font-semibold">1.</span>
                  <span>Upload an audio/video file (MP3, WAV, M4A, MP4, WebM)</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">2.</span>
                  <span>Play the audio and watch real-time transcription</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">3.</span>
                  <span>See AI-powered conversation guidance based on the dialogue</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">4.</span>
                  <span>Follow suggested paths to optimize the conversation outcome</span>
                </li>
              </ol>
            </div>
          </div>
          </div>
        )}

        {/* Main Demo Interface */}
        {audioFile && (
          <div className="grid lg:grid-cols-2 gap-6 flex-1 min-h-0">
            {/* Left Column: Audio Player & Transcript */}
            <div className="flex flex-col gap-6 min-h-0">
              <div className="flex-shrink-0">
                <AudioPlaybackSimulator
                  audioFile={audioFile}
                  onTranscriptUpdate={handleTranscriptUpdate}
                  onPlaybackStateChange={setIsPlaying}
                  simulateRealtime={true}
                  chunkDuration={2}
                />
              </div>

              <div className="flex-1 min-h-0">
                <LiveTranscript
                  entries={transcriptEntries}
                  isListening={isPlaying}
                  currentTime={currentTime}
                  conversationHealth={conversationHealth}
                  currentStage={agentState.currentStage || stages.find(s => s.status === "current")?.label || "In Progress"}
                  autoScroll={true}
                  showConfidence={true}
                />
              </div>
            </div>

            {/* Right Column: AI Copilot */}
            <div className="min-h-0">
              <AgentCopilot
                stages={stages}
                currentStep={currentStepData}
                conversationHealth={conversationHealth}
                onActionClick={handleActionClick}
                onFeedback={handleFeedback}
              />
            </div>
          </div>
        )}
        </div>
      </div>
    </div>
  )
}
