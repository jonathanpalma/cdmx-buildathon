import type { Route } from "./+types/_index";
import { useState, useCallback, useMemo, useRef, useEffect } from "react"
import { AudioUpload } from "~/components/audio/audio-upload"
import { AudioPlaybackSimulator } from "~/components/audio/audio-playback-simulator"
import { LiveTranscriptImproved as LiveTranscript, type TranscriptEntry } from "~/components/audio/live-transcript-improved"
import { AgentCopilot, type CurrentStepData } from "~/components/copilot"
import { AgentCopilotV2 } from "~/components/copilot/agent-copilot-v2"
import type { AgentState, ConversationStage, ExecutableAction, CustomerProfile } from "~/lib/agent/state"
import { logger } from "~/lib/logger.client"
import { validateEnvironment } from "~/lib/env-validation.server"

export function meta({}: Route.MetaArgs) {
  return [
    { title: "OneVoice - AI Call Center Copilot" },
    { name: "description", content: "AI-powered copilot for call center agents" },
  ];
}

export async function loader() {
  // Validate environment variables on page load
  const envValidation = validateEnvironment()

  return {
    envValidation
  }
}

export default function Index() {
  const [audioFile, setAudioFile] = useState<File | null>(null)
  const [transcriptEntries, setTranscriptEntries] = useState<TranscriptEntry[]>([])
  const [currentTime, setCurrentTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [conversationHealth, setConversationHealth] = useState(75)

  // AI Agent state (replaces static currentStepData)
  const [agentState, setAgentState] = useState<Partial<AgentState>>({})
  const [isAgentProcessing, setIsAgentProcessing] = useState(false)
  const [forceUpdate, setForceUpdate] = useState(false) // Flag to force UI update
  const [suggestionTimestamp, setSuggestionTimestamp] = useState<number>(0) // When current suggestion was generated

  // Agent call management (debouncing + cancellation + race condition prevention)
  const abortControllerRef = useRef<AbortController | null>(null)
  const debounceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const maxWaitTimerRef = useRef<NodeJS.Timeout | null>(null)
  const pendingMessagesRef = useRef<TranscriptEntry[]>([])
  const requestIdRef = useRef<number>(0) // Monotonically increasing request ID
  const lastProcessedRequestRef = useRef<number>(0) // Track last successfully processed request

  // Auto-execution state
  const [autoExecuteCountdown, setAutoExecuteCountdown] = useState<{
    actionId: string
    remaining: number
  } | null>(null)
  const autoExecuteTimerRef = useRef<NodeJS.Timeout | null>(null)

  // Cleanup timers on unmount
  useEffect(() => {
    return () => {
      if (debounceTimerRef.current) clearTimeout(debounceTimerRef.current)
      if (maxWaitTimerRef.current) clearTimeout(maxWaitTimerRef.current)
      if (autoExecuteTimerRef.current) clearTimeout(autoExecuteTimerRef.current)
      if (abortControllerRef.current) abortControllerRef.current.abort()
    }
  }, [])

  // Conversation stages come directly from agent (dynamically generated)
  const stages = useMemo<ConversationStage[]>(() => {
    return agentState.conversationStages || []
  }, [agentState.conversationStages])

  // Current step data - use AI agent state if available, otherwise show empty state
  const currentStepData = useMemo<CurrentStepData | null>(() => {
    // Don't show anything until we have transcript entries
    if (transcriptEntries.length === 0) {
      return null
    }

    // If agent has generated suggestions, use them
    if (agentState.nextActions && agentState.nextActions.length > 0) {
      const currentStage = stages.find(s => s.status === "current")

      return {
        stage: agentState.currentStage || currentStage?.label || "In Progress",
        description: currentStage?.description || "AI is analyzing the conversation...",
        aiSuggestion: agentState.reasoning || undefined,
        nextActions: agentState.nextActions,
        script: agentState.nextActions[0]?.description,
        tips: [], // Tips can be generated by agent if needed
      }
    }

    // If no agent data yet, return null (shows waiting state in copilot)
    return null
  }, [agentState, stages, transcriptEntries.length])

  // Handle file selection
  const handleFileSelect = useCallback((file: File) => {
    setAudioFile(file)
    setTranscriptEntries([])
    setIsPlaying(false)
    setConversationHealth(75)
    setAgentState({}) // Reset agent state
  }, [])

  // Execute agent call with accumulated messages
  const executeAgentCall = useCallback(async (messages: TranscriptEntry[]) => {
    // Don't start a new request if one is already in progress
    // Let it complete to avoid wasted API calls
    if (isAgentProcessing) {
      logger.debug("Agent call already in progress, skipping this request", {
        pendingMessageCount: messages.length
      })
      return
    }

    // Wait for minimum message context before first agent call
    // This prevents premature analysis on initial greetings
    const MIN_MESSAGES_FOR_FIRST_CALL = 3
    const hasNeverAnalyzed = !agentState.currentStage && !agentState.executableActions

    if (hasNeverAnalyzed && messages.length < MIN_MESSAGES_FOR_FIRST_CALL) {
      logger.debug("â¸ï¸ WAITING_FOR_CONTEXT", {
        messageCount: messages.length,
        required: MIN_MESSAGES_FOR_FIRST_CALL,
        reason: "first_call_threshold"
      })
      return
    }

    // Assign monotonically increasing request ID for race condition detection
    const currentRequestId = ++requestIdRef.current

    abortControllerRef.current = new AbortController()
    setIsAgentProcessing(true)

    try {
      // Get the last message (most recent)
      const lastMessage = messages[messages.length - 1]

      logger.debug("ðŸš€ Agent call started", {
        requestId: currentRequestId,
        messageCount: messages.length,
        lastProcessed: lastProcessedRequestRef.current
      })

      const response = await fetch("/api/agent", {
        signal: abortControllerRef.current.signal,
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          message: {
            speaker: lastMessage.speaker,
            text: lastMessage.text,
            timestamp: lastMessage.timestamp,
          },
          currentState: agentState,
          requestId: currentRequestId, // Include request ID in payload
        }),
      })

      if (response.ok) {
        const { state } = await response.json()

        // ðŸ›¡ï¸ RACE CONDITION PROTECTION
        // Only apply response if this is the latest request
        if (currentRequestId < lastProcessedRequestRef.current) {
          logger.warn("âš ï¸ STALE_RESPONSE_DISCARDED", {
            responseRequestId: currentRequestId,
            latestProcessed: lastProcessedRequestRef.current,
            messageCount: messages.length,
            reason: "Older response arrived after newer one was processed"
          })
          return // Discard this stale response
        }

        // Update last processed request ID
        lastProcessedRequestRef.current = currentRequestId

        logger.info("âœ… RESPONSE_ACCEPTED", {
          requestId: currentRequestId,
          messageCount: messages.length
        })

        // Check if we should force update (manual refresh)
        const shouldForceUpdate = forceUpdate

        // Check if we have current suggestions showing (new format)
        const hasCurrentSuggestions = agentState.executableActions && agentState.executableActions.length > 0
        const hasNewSuggestions = state.executableActions && state.executableActions.length > 0
        const stageChanged = state.currentStage !== agentState.currentStage

        // Determine update decision
        const willUpdate = !hasCurrentSuggestions || stageChanged || shouldForceUpdate || hasNewSuggestions
        const updateReason = !hasCurrentSuggestions ? "no_current" :
                             stageChanged ? "stage_changed" :
                             shouldForceUpdate ? "manual_refresh" : "new_suggestions"

        logger.info("ðŸ“Š UI_UPDATE_DECISION", {
          requestId: currentRequestId,
          willUpdate,
          reason: updateReason,
          before: {
            actions: agentState.executableActions?.length || 0,
            stage: agentState.currentStage,
            hasInsights: !!agentState.insights
          },
          after: {
            actions: state.executableActions?.length || 0,
            stage: state.currentStage,
            hasInsights: !!state.insights,
            topAction: state.executableActions?.[0] ? {
              label: state.executableActions[0].label,
              confidence: state.executableActions[0].confidence
            } : null
          }
        })

        // ALWAYS update state, but be smart about what to show
        // This ensures insights are always fresh, even if no actions
        if (willUpdate) {
          setAgentState(state)
          setConversationHealth(state.healthScore || 75)

          // Only update timestamp if we have new executable actions
          if (hasNewSuggestions) {
            setSuggestionTimestamp(Date.now())
          }
          setForceUpdate(false) // Reset flag
        } else {
          logger.info("ðŸ“Œ KEEPING_CURRENT_UI", {
            requestId: currentRequestId,
            reason: "Actions still relevant, updating background only"
          })
          // Update insights and context but keep current actions visible
          setAgentState(prev => ({
            ...state,
            executableActions: prev.executableActions, // Keep current actions visible
          }))
          setConversationHealth(state.healthScore || 75)
        }
      }
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        logger.debug("Agent request cancelled (newer message arrived)")
        return
      }
      logger.error("Agent call failed", { error })
    } finally {
      setIsAgentProcessing(false)
    }
  }, [agentState])

  // Smart batching: Detect incomplete sentences, critical info, and urgency
  const analyzeMessageCompleteness = useCallback((text: string) => {
    const lowerText = text.toLowerCase().trim()

    // Incomplete sentence indicators
    const incompletePatterns = [
      /\b(til|till|until|to|through|and|or|with|from)\s*$/i,  // Ends with connector
      /\b(may|june|july|august|april|march|january|february|september|october|november|december)\s+\d{1,2}\s*$/i,  // Date started but not complete
      /\d{1,2}(st|nd|rd|th)?\s*$/i,  // Number at end (might be followed by more info)
      /\b(check\s*in|checking\s*in|arrive|arriving)\s*$/i,  // Started talking about dates
      /\b(adults?|children?|kids?|people?|guests?)\s*$/i,  // Started party size info
    ]

    const isIncomplete = incompletePatterns.some(pattern => pattern.test(lowerText))

    // Critical info patterns that suggest important data is being shared
    const criticalPatterns = [
      /\b(may|june|july|august|april|march|january|february|september|october|november|december)\s+\d{1,2}/i,  // Date mention
      /\d{1,2}(st|nd|rd|th)/i,  // Ordinal dates
      /\b\d+\s+(adults?|children?|kids?|people?|guests?)\b/i,  // Party size
      /\$([\d,]+)/i,  // Price/budget
    ]

    const hasCriticalInfo = criticalPatterns.some(pattern => pattern.test(lowerText))

    // Urgency detection - process these FAST
    const urgencyPatterns = [
      /\b(urgent|asap|immediately|right\s+now|today)\b/i,
      /\b(need\s+it|want\s+it)\s+(now|today|asap)/i,
      /\b(leaving|departing)\s+(tomorrow|today|soon)/i,
      /\b(running\s+out\s+of\s+time|time\s+sensitive)/i
    ]

    const isUrgent = urgencyPatterns.some(pattern => pattern.test(lowerText))

    // Short confirmations - also process fast
    const confirmationPatterns = [
      /^\s*(yes|yeah|yep|yup|sure|okay|ok|correct|right|exactly)\s*$/i,
      /^\s*(that('s|\s+is)\s+(correct|right|perfect))\s*$/i,
      /^\s*(sounds?\s+(good|great|perfect))\s*$/i
    ]

    const isShortConfirmation = confirmationPatterns.some(pattern => pattern.test(lowerText)) &&
                                text.split(' ').length <= 3

    return {
      isIncomplete,
      hasCriticalInfo,
      isUrgent,
      isShortConfirmation
    }
  }, [])

  // Call AI Agent with smart batching + debouncing
  const callAgent = useCallback((newEntry: TranscriptEntry) => {
    // Add to pending messages
    pendingMessagesRef.current.push(newEntry)

    // Analyze message characteristics
    const { isIncomplete, hasCriticalInfo, isUrgent, isShortConfirmation } =
      analyzeMessageCompleteness(newEntry.text)

    logger.debug("Agent message queued", {
      pendingCount: pendingMessagesRef.current.length,
      isIncomplete,
      hasCriticalInfo,
      isUrgent,
      isShortConfirmation,
      lastWords: newEntry.text.slice(-30)
    })

    // Cancel existing debounce timer
    if (debounceTimerRef.current) {
      logger.debug("Agent resetting debounce timer")
      clearTimeout(debounceTimerRef.current)
    }

    // Set up max wait timer (force execution after 8s even if still receiving messages)
    if (!maxWaitTimerRef.current) {
      logger.debug("Agent starting max wait timer (8s)")
      maxWaitTimerRef.current = setTimeout(() => {
        logger.debug("Agent max wait timer triggered - forcing execution")
        if (debounceTimerRef.current) {
          clearTimeout(debounceTimerRef.current)
        }
        const messages = [...pendingMessagesRef.current]
        pendingMessagesRef.current = []
        maxWaitTimerRef.current = null
        executeAgentCall(messages)
      }, 8000) // Max 8 seconds wait
    }

    // ðŸš€ ADAPTIVE TIMING: Adjust based on message characteristics
    let debounceDelay = 2500 // Default: 2.5 seconds

    if (isUrgent) {
      // URGENT requests - process almost immediately
      debounceDelay = 500 // 0.5 seconds
      logger.debug("ðŸ”¥ URGENT_DETECTED - immediate processing", {
        delay: debounceDelay,
        text: newEntry.text.slice(0, 50)
      })
    } else if (isShortConfirmation) {
      // Short confirmations - quick response expected
      debounceDelay = 1000 // 1 second
      logger.debug("âœ… CONFIRMATION_DETECTED - fast response", {
        delay: debounceDelay
      })
    } else if (isIncomplete && hasCriticalInfo) {
      // Critical info but incomplete - wait for completion
      debounceDelay = 3500 // 3.5 seconds
      logger.debug("â³ CRITICAL_INCOMPLETE - waiting for completion", {
        delay: debounceDelay
      })
    } else if (isIncomplete) {
      // General incomplete sentence - wait longer
      debounceDelay = 4000 // 4 seconds
      logger.debug("â³ Incomplete sentence - extending wait time", {
        delay: debounceDelay
      })
    } else if (hasCriticalInfo && !isIncomplete) {
      // Complete sentence with important info - process faster
      debounceDelay = 1500 // 1.5 seconds
      logger.debug("âš¡ Complete critical info - fast-tracking", {
        delay: debounceDelay
      })
    }

    // Debounce: wait for silence before calling agent
    debounceTimerRef.current = setTimeout(() => {
      logger.debug("Agent debounce timer triggered - executing", {
        batchSize: pendingMessagesRef.current.length,
        finalDelay: debounceDelay
      })
      if (maxWaitTimerRef.current) {
        clearTimeout(maxWaitTimerRef.current)
        maxWaitTimerRef.current = null
      }
      const messages = [...pendingMessagesRef.current]
      pendingMessagesRef.current = []
      executeAgentCall(messages)
    }, debounceDelay)
  }, [executeAgentCall, analyzeMessageCompleteness])

  // Buffer for handling out-of-order responses
  const pendingTranscriptsRef = useRef<Map<number, any>>(new Map())
  const nextExpectedSequenceRef = useRef(0)

  // Handle transcript updates from playback
  const handleTranscriptUpdate = useCallback((event: any) => {
    setCurrentTime(event.timestamp)

    // Handle sequence-based ordering to prevent race conditions
    const sequenceNumber = event.sequenceNumber ?? event.chunkIndex

    logger.debug("Transcript event received", {
      sequence: sequenceNumber,
      chunkIndex: event.chunkIndex,
      isEmpty: event.isEmpty,
      textLength: event.text?.length || 0,
      speaker: event.speaker,
      nextExpected: nextExpectedSequenceRef.current,
      bufferSize: pendingTranscriptsRef.current.size
    })

    // Skip empty transcriptions (silence or API returned no text)
    // BUT still process them to maintain sequence continuity
    if (!event.text || !event.text.trim() || event.isEmpty) {
      logger.debug("Empty transcript, advancing sequence", { sequence: sequenceNumber })
      // Add to buffer to maintain sequence continuity
      pendingTranscriptsRef.current.set(sequenceNumber, { ...event, isEmpty: true })
    } else {
      // Add to pending buffer
      pendingTranscriptsRef.current.set(sequenceNumber, event)
    }

    // Process all sequential entries starting from next expected
    const processSequentialEntries = () => {
      let processedCount = 0
      while (pendingTranscriptsRef.current.has(nextExpectedSequenceRef.current)) {
        const nextEvent = pendingTranscriptsRef.current.get(nextExpectedSequenceRef.current)!

        // Skip processing empty events, just advance the sequence
        if (nextEvent.isEmpty || !nextEvent.text || !nextEvent.text.trim()) {
          logger.debug("Skipping empty sequence", {
            sequence: nextExpectedSequenceRef.current
          })
          pendingTranscriptsRef.current.delete(nextExpectedSequenceRef.current)
          nextExpectedSequenceRef.current++
          continue
        }
        pendingTranscriptsRef.current.delete(nextExpectedSequenceRef.current)
        nextExpectedSequenceRef.current++
        processedCount++

        // Add to transcript
        setTranscriptEntries((prev) => {
          // If this chunk has speaker segments (mid-chunk speaker changes), process them separately
          if (nextEvent.segments && nextEvent.segments.length > 1) {
            logger.debug("Processing chunk with multiple speaker segments", {
              chunkIndex: nextEvent.chunkIndex,
              segmentCount: nextEvent.segments.length,
              segments: nextEvent.segments.map((s: any) => ({
                speaker: s.speaker,
                text: s.text.substring(0, 20) + '...'
              }))
            })

            // Map speaker IDs (0, 1) to role labels (agent, customer)
            const mapSpeaker = (speakerId: number): "agent" | "customer" => {
              // Channel 0 = Agent (left channel)
              // Channel 1 = Customer (right channel)
              return speakerId === 0 ? "agent" : "customer"
            }

            // Create separate entries for each speaker segment
            let newEntries = [...prev]
            for (const segment of nextEvent.segments) {
              const segmentSpeaker = mapSpeaker(segment.speaker)
              const lastEntry = newEntries[newEntries.length - 1]

              // Check if we should merge with last entry
              const shouldMerge =
                lastEntry &&
                lastEntry.speaker === segmentSpeaker &&
                segment.startTime - lastEntry.timestamp < 5

              if (shouldMerge) {
                // Merge with previous segment
                const mergedEntry: TranscriptEntry = {
                  ...lastEntry,
                  text: `${lastEntry.text} ${segment.text}`,
                  confidence: (lastEntry.confidence! + segment.confidence) / 2,
                }
                newEntries = [...newEntries.slice(0, -1), mergedEntry]
              } else {
                // Create new entry for this segment
                const segmentEntry: TranscriptEntry = {
                  id: `segment-${nextEvent.chunkIndex}-${segment.speaker}-${segment.startTime}`,
                  timestamp: segment.startTime,
                  text: segment.text,
                  speaker: segmentSpeaker,
                  confidence: segment.confidence,
                  isFinal: true,
                }
                newEntries.push(segmentEntry)

                // Call agent when speaker changes
                if (lastEntry && lastEntry.speaker !== segmentSpeaker) {
                  callAgent(lastEntry)
                } else if (!lastEntry) {
                  callAgent(segmentEntry)
                }
              }
            }
            return newEntries
          }

          // No segments or single speaker - process as before
          const exists = prev.some(
            (entry) =>
              entry.timestamp === nextEvent.timestamp && entry.text === nextEvent.text
          )

          if (exists) {
            return prev
          }

          // Check if we should merge with the last entry from the same speaker
          const lastEntry = prev[prev.length - 1]
          const shouldMerge =
            lastEntry &&
            lastEntry.speaker === nextEvent.speaker &&
            nextEvent.timestamp - lastEntry.timestamp < 5 // Merge if within 5 seconds

          if (shouldMerge) {
            // Create merged entry
            const mergedEntry: TranscriptEntry = {
              ...lastEntry,
              text: `${lastEntry.text} ${nextEvent.text}`,
              confidence: (lastEntry.confidence! + nextEvent.confidence) / 2, // Average confidence
            }

            // DON'T call agent yet - we're still accumulating from same speaker
            // The agent will be called when speaker changes or after a pause

            // Replace last entry with merged one
            return [...prev.slice(0, -1), mergedEntry]
          }

          // Create new entry (new speaker or time gap)
          const newEntry: TranscriptEntry = {
            id: `entry-${nextEvent.chunkIndex}-${nextEvent.timestamp}`,
            timestamp: nextEvent.timestamp,
            text: nextEvent.text,
            speaker: nextEvent.speaker,
            confidence: nextEvent.confidence,
            isFinal: true,
          }

          const newEntries = [...prev, newEntry]

          // Call AI agent when speaker changes or significant gap
          // This ensures we send complete thoughts, not fragments
          if (lastEntry && lastEntry.speaker !== nextEvent.speaker) {
            // Speaker changed - send the PREVIOUS speaker's complete message
            callAgent(lastEntry)
          } else if (!lastEntry) {
            // First message - send it
            callAgent(newEntry)
          }
          // If same speaker but time gap, the debounce will handle it

          return newEntries
        })
      }

      if (processedCount > 0) {
        logger.debug("Processed sequential entries", {
          count: processedCount,
          nextExpected: nextExpectedSequenceRef.current,
          remainingBuffer: pendingTranscriptsRef.current.size
        })
      }

      // Log buffer state if there are pending items
      if (pendingTranscriptsRef.current.size > 0) {
        const bufferedSequences = Array.from(pendingTranscriptsRef.current.keys()).sort((a, b) => a - b)
        logger.debug("Sequence buffer status", {
          nextExpected: nextExpectedSequenceRef.current,
          buffered: bufferedSequences,
          gap: bufferedSequences[0] - nextExpectedSequenceRef.current
        })
      }
    }

    processSequentialEntries()
  }, [callAgent])

  // Execute MCP tool
  const executeMCPAction = useCallback(async (action: ExecutableAction) => {
    const startTime = Date.now()

    // Merge customer profile data with action parameters
    // Customer profile is the source of truth - use it to fill in any missing parameters
    const enrichedParameters = {
      ...action.parameters,
      // Override with customer profile data if available
      ...(agentState.customerProfile?.travelDates?.checkIn && {
        checkIn: agentState.customerProfile.travelDates.checkIn
      }),
      ...(agentState.customerProfile?.travelDates?.checkOut && {
        checkOut: agentState.customerProfile.travelDates.checkOut
      }),
      ...(agentState.customerProfile?.partySize?.adults && {
        adults: agentState.customerProfile.partySize.adults
      }),
      ...(agentState.customerProfile?.partySize?.children && {
        children: agentState.customerProfile.partySize.children
      }),
      ...(agentState.customerProfile?.budget?.max && {
        maxBudget: agentState.customerProfile.budget.max
      }),
      ...(agentState.customerProfile?.preferences && agentState.customerProfile.preferences.length > 0 && {
        preferences: agentState.customerProfile.preferences
      }),
    }

    logger.info("ðŸš€ MCP_EXEC_START", {
      actionId: action.id,
      label: action.label,
      toolName: action.toolName,
      parameters: enrichedParameters,
      originalParams: action.parameters,
      enrichedFromProfile: Object.keys(enrichedParameters).filter(k => !(k in (action.parameters || {}))),
      confidence: action.confidence,
      triggeredBy: autoExecuteCountdown ? "auto_exec" : "manual_click"
    })

    // Update action status to executing
    setAgentState(prev => ({
      ...prev,
      executableActions: prev.executableActions?.map(a =>
        a.id === action.id ? { ...a, status: "executing" as const } : a
      ),
      // Add to background tasks
      backgroundTasks: [
        ...(prev.backgroundTasks || []),
        {
          id: `task-${Date.now()}`,
          label: action.label,
          type: action.executionType,
          toolName: action.toolName,
          parameters: enrichedParameters,
          status: "running",
          startedAt: Date.now(),
        }
      ]
    }))

    try {
      // Call MCP execution API with enriched parameters
      const response = await fetch("/api/mcp/execute", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          actionId: action.id,
          toolName: action.toolName,
          parameters: enrichedParameters,
        }),
      })

      const result = await response.json()

      if (result.success) {
        logger.info("âœ… MCP_EXEC_COMPLETE", {
          actionId: action.id,
          label: action.label,
          toolName: action.toolName,
          success: true,
          summary: result.summary,
          duration: Date.now() - startTime + "ms",
          dataKeys: result.data ? Object.keys(result.data) : []
        })

        // Update action status to completed
        setAgentState(prev => ({
          ...prev,
          executableActions: prev.executableActions?.map(a =>
            a.id === action.id
              ? { ...a, status: "completed" as const, result: result.summary }
              : a
          ),
          // Update background task
          backgroundTasks: prev.backgroundTasks?.map(t =>
            t.label === action.label && t.status === "running"
              ? {
                  ...t,
                  status: "completed",
                  completedAt: Date.now(),
                  result: {
                    summary: result.summary,
                    data: result.data,
                  },
                  progress: 100,
                }
              : t
          ),
        }))
      } else {
        logger.error("âŒ MCP_EXEC_FAILED", {
          actionId: action.id,
          label: action.label,
          toolName: action.toolName,
          error: result.error,
          duration: Date.now() - startTime + "ms"
        })

        // Update action status to failed
        setAgentState(prev => ({
          ...prev,
          executableActions: prev.executableActions?.map(a =>
            a.id === action.id
              ? { ...a, status: "failed" as const, error: result.error }
              : a
          ),
          // Update background task
          backgroundTasks: prev.backgroundTasks?.map(t =>
            t.label === action.label && t.status === "running"
              ? {
                  ...t,
                  status: "failed",
                  completedAt: Date.now(),
                  error: result.error,
                }
              : t
          ),
        }))
      }
    } catch (error) {
      logger.error("MCP action execution error", { actionId: action.id, error })

      // Update action status to failed
      setAgentState(prev => ({
        ...prev,
        executableActions: prev.executableActions?.map(a =>
          a.id === action.id
            ? { ...a, status: "failed" as const, error: String(error) }
            : a
        ),
        // Update background task
        backgroundTasks: prev.backgroundTasks?.map(t =>
          t.label === action.label && t.status === "running"
            ? {
                ...t,
                status: "failed",
                completedAt: Date.now(),
                error: String(error),
              }
            : t
        ),
      }))
    }
  }, [agentState.customerProfile, autoExecuteCountdown])

  // Auto-execution logic - detect 95%+ confidence actions
  useEffect(() => {
    const autoExecutableAction = agentState.executableActions?.find(
      action =>
        action.confidence >= 95 &&
        !action.requiresConfirmation &&
        action.status === "suggested"
    )

    if (autoExecutableAction && !autoExecuteCountdown) {
      logger.info("âš¡ AUTO_EXEC_DETECTED", {
        actionId: autoExecutableAction.id,
        label: autoExecutableAction.label,
        confidence: autoExecutableAction.confidence,
        toolName: autoExecutableAction.toolName,
        requiresConfirmation: autoExecutableAction.requiresConfirmation,
        riskLevel: autoExecutableAction.riskLevel,
        countdownSeconds: 3,
        canCancel: true
      })

      // Start 3-second countdown
      let remaining = 3
      setAutoExecuteCountdown({
        actionId: autoExecutableAction.id,
        remaining,
      })

      const countdownInterval = setInterval(() => {
        remaining--
        if (remaining > 0) {
          setAutoExecuteCountdown({
            actionId: autoExecutableAction.id,
            remaining,
          })
          logger.debug("â±ï¸ AUTO_EXEC_COUNTDOWN", { remaining })
        } else {
          // Execute the action
          clearInterval(countdownInterval)
          setAutoExecuteCountdown(null)
          logger.info("âœ… AUTO_EXEC_TRIGGERED", {
            actionId: autoExecutableAction.id,
            label: autoExecutableAction.label
          })
          executeMCPAction(autoExecutableAction)
        }
      }, 1000)

      autoExecuteTimerRef.current = countdownInterval

      // Return cleanup function
      return () => {
        if (autoExecuteTimerRef.current) {
          clearInterval(autoExecuteTimerRef.current)
          autoExecuteTimerRef.current = null
        }
      }
    }
  }, [agentState.executableActions, autoExecuteCountdown, executeMCPAction])

  const handleActionClick = useCallback((actionId: string) => {
    // Find the action
    const action = agentState.executableActions?.find(a => a.id === actionId)
    if (!action) {
      logger.warn("Action not found", { actionId })
      return
    }

    logger.info("ðŸ‘† USER_ACTION", {
      type: "action_click",
      actionId,
      actionLabel: action.label,
      confidence: action.confidence,
      toolName: action.toolName,
      wasAutoCancelled: autoExecuteCountdown?.actionId === actionId
    })

    // Cancel any auto-execution countdown
    if (autoExecuteCountdown?.actionId === actionId) {
      if (autoExecuteTimerRef.current) {
        clearInterval(autoExecuteTimerRef.current)
        autoExecuteTimerRef.current = null
      }
      setAutoExecuteCountdown(null)
      logger.info("ðŸ›‘ AUTO_EXEC_CANCELLED", { reason: "user_click", actionId })
    }

    // Execute the action
    executeMCPAction(action)
  }, [agentState.executableActions, autoExecuteCountdown, executeMCPAction])

  const handleActionCancel = useCallback((actionId: string) => {
    const action = agentState.executableActions?.find(a => a.id === actionId)

    logger.info("ðŸ‘† USER_ACTION", {
      type: "action_cancel",
      actionId,
      actionLabel: action?.label,
      wasAutoExecuting: autoExecuteCountdown?.actionId === actionId
    })

    // Cancel auto-execution countdown if active
    if (autoExecuteCountdown?.actionId === actionId) {
      if (autoExecuteTimerRef.current) {
        clearInterval(autoExecuteTimerRef.current)
        autoExecuteTimerRef.current = null
      }
      setAutoExecuteCountdown(null)
      logger.info("ðŸ›‘ AUTO_EXEC_CANCELLED", { reason: "user_cancel", actionId })
    }

    // Remove the action from suggestions
    setAgentState(prev => ({
      ...prev,
      executableActions: prev.executableActions?.filter(a => a.id !== actionId),
    }))
  }, [agentState.executableActions, autoExecuteCountdown])

  const handleScriptCopy = useCallback((scriptId: string) => {
    // Find the script
    const script = agentState.quickScripts?.find(s => s.id === scriptId)
    if (!script) {
      logger.warn("Script not found", { scriptId })
      return
    }

    logger.info("ðŸ‘† USER_ACTION", {
      type: "script_copy",
      scriptId,
      scriptLabel: script.label,
      confidence: script.confidence,
      scriptLength: script.script.length
    })

    // Copy to clipboard
    navigator.clipboard.writeText(script.script).then(() => {
      logger.info("ðŸ“‹ SCRIPT_COPIED", {
        scriptId,
        label: script.label
      })
      // TODO: Show toast notification
    }).catch((error) => {
      logger.error("âŒ SCRIPT_COPY_FAILED", { scriptId, error })
    })
  }, [agentState.quickScripts])

  const handleFeedback = useCallback((actionId: string, positive: boolean) => {
    // TODO: Track agent feedback for ML improvement
    logger.info("Feedback received", { actionId, positive })
  }, [])

  const handleProfileUpdate = useCallback((updates: Partial<CustomerProfile>) => {
    logger.info("ðŸ‘¤ PROFILE_UPDATE", {
      type: "manual_override",
      updates: Object.keys(updates),
      hasNewDates: !!updates.travelDates,
      hasNewPartySize: !!updates.partySize,
      hasNewBudget: !!updates.budget
    })

    // Update agent state with manual overrides
    setAgentState(prev => ({
      ...prev,
      customerProfile: {
        ...prev.customerProfile,
        ...updates
      }
    }))
  }, [])

  const handleRefresh = useCallback(() => {
    logger.debug("Agent manual refresh requested")

    // Set force update flag to bypass UI update logic
    setForceUpdate(true)

    // Clear current suggestions to allow new ones
    setAgentState(prev => ({
      ...prev,
      nextActions: [],
      reasoning: "",
    }))

    // Force-call agent immediately with last message (bypass debounce)
    if (transcriptEntries.length > 0) {
      const lastEntry = transcriptEntries[transcriptEntries.length - 1]

      // Cancel any pending debounce timers
      if (debounceTimerRef.current) {
        clearTimeout(debounceTimerRef.current)
        debounceTimerRef.current = null
      }
      if (maxWaitTimerRef.current) {
        clearTimeout(maxWaitTimerRef.current)
        maxWaitTimerRef.current = null
      }

      // Clear pending messages and execute immediately
      pendingMessagesRef.current = []
      executeAgentCall([lastEntry])
    }
  }, [transcriptEntries, executeAgentCall])

  return (
    <div className="h-screen bg-gray-50 flex flex-col overflow-hidden">
      {/* Header */}
      <div className="bg-white border-b flex-shrink-0">
        <div className="max-w-7xl mx-auto px-4 py-3">
          <div className="flex items-center justify-between">
            <div className="flex items-center gap-3">
              <h1 className="text-xl font-bold text-gray-900">OneVoice</h1>
              <span className="text-sm text-gray-500">AI Call Center Copilot</span>
            </div>
            {audioFile && (
              <button
                type="button"
                onClick={() => {
                  setAudioFile(null)
                  setTranscriptEntries([])
                  setIsPlaying(false)
                  setConversationHealth(75)
                  setAgentState({})
                }}
                className="px-3 py-1.5 text-sm text-gray-600 hover:text-gray-900 border rounded-lg hover:bg-gray-50"
              >
                Upload New Audio
              </button>
            )}
          </div>
        </div>
      </div>

      <div className="flex-1 min-h-0 overflow-hidden">
        <div className="max-w-7xl mx-auto px-4 py-6 h-full flex flex-col">
        {/* Audio Upload Section */}
        {!audioFile && (
          <div className="flex-1 overflow-auto">
          <div className="max-w-2xl mx-auto">
            <div className="text-center mb-8">
              <h2 className="text-2xl font-semibold text-gray-900 mb-3">
                Upload a Call Recording
              </h2>
              <p className="text-gray-600">
                Upload an audio or video file to see real-time transcription with AI-powered conversation guidance
              </p>
            </div>
            <AudioUpload onFileSelect={handleFileSelect} />

            <div className="mt-8 bg-blue-50 border border-blue-200 rounded-lg p-6">
              <h3 className="font-semibold text-blue-900 mb-3">How it works:</h3>
              <ol className="space-y-2 text-sm text-blue-800">
                <li className="flex gap-2">
                  <span className="font-semibold">1.</span>
                  <span>Upload an audio/video file (MP3, WAV, M4A, MP4, WebM)</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">2.</span>
                  <span>Play the audio and watch real-time transcription</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">3.</span>
                  <span>See AI-powered conversation guidance based on the dialogue</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">4.</span>
                  <span>Follow suggested paths to optimize the conversation outcome</span>
                </li>
              </ol>
            </div>
          </div>
          </div>
        )}

        {/* Main Demo Interface */}
        {audioFile && (
          <div className="grid lg:grid-cols-2 gap-6 flex-1 min-h-0">
            {/* Left Column: Audio Player & Transcript */}
            <div className="flex flex-col gap-6 min-h-0">
              <div className="flex-shrink-0">
                <AudioPlaybackSimulator
                  audioFile={audioFile}
                  onTranscriptUpdate={handleTranscriptUpdate}
                  onPlaybackStateChange={setIsPlaying}
                  simulateRealtime={true}
                  chunkDuration={2}
                />
              </div>

              <div className="flex-1 min-h-0">
                <LiveTranscript
                  entries={transcriptEntries}
                  isListening={isPlaying}
                  currentTime={currentTime}
                  conversationHealth={conversationHealth}
                  currentStage={agentState.currentStage || stages.find(s => s.status === "current")?.label || "In Progress"}
                  autoScroll={true}
                  showConfidence={true}
                />
              </div>
            </div>

            {/* Right Column: AI Copilot V2 */}
            <div className="min-h-0">
              <AgentCopilotV2
                executableActions={agentState.executableActions || []}
                insights={agentState.insights}
                quickScripts={agentState.quickScripts || []}
                backgroundTasks={agentState.backgroundTasks || []}
                stages={stages}
                customerProfile={agentState.customerProfile || {}}
                onActionClick={handleActionClick}
                onActionCancel={handleActionCancel}
                onScriptCopy={handleScriptCopy}
                onProfileUpdate={handleProfileUpdate}
                isProcessing={isAgentProcessing}
                messageCount={transcriptEntries.length}
                hasAnalyzedBefore={!!agentState.currentStage || !!agentState.executableActions}
                isListening={isPlaying}
              />
            </div>
          </div>
        )}
        </div>
      </div>
    </div>
  )
}
