import type { Route } from "./+types/_index";
import { useState, useCallback, useMemo, useRef, useEffect } from "react"
import { AudioUpload } from "~/components/audio/audio-upload"
import { AudioPlaybackSimulator } from "~/components/audio/audio-playback-simulator"
import { LiveTranscriptImproved as LiveTranscript, type TranscriptEntry } from "~/components/audio/live-transcript-improved"
import { AgentCopilot, type CurrentStepData } from "~/components/copilot"
import type { AgentState, ConversationStage } from "~/lib/agent/state"
import { logger } from "~/lib/logger.client"
import { validateEnvironment } from "~/lib/env-validation.server"

export function meta({}: Route.MetaArgs) {
  return [
    { title: "CloseLoop - AI Call Center Copilot" },
    { name: "description", content: "AI-powered copilot for call center agents" },
  ];
}

export async function loader() {
  // Validate environment variables on page load
  const envValidation = validateEnvironment()

  return {
    envValidation
  }
}

export default function Index() {
  const [audioFile, setAudioFile] = useState<File | null>(null)
  const [transcriptEntries, setTranscriptEntries] = useState<TranscriptEntry[]>([])
  const [currentTime, setCurrentTime] = useState(0)
  const [isPlaying, setIsPlaying] = useState(false)
  const [conversationHealth, setConversationHealth] = useState(75)

  // AI Agent state (replaces static currentStepData)
  const [agentState, setAgentState] = useState<Partial<AgentState>>({})
  const [isAgentProcessing, setIsAgentProcessing] = useState(false)
  const [forceUpdate, setForceUpdate] = useState(false) // Flag to force UI update
  const [suggestionTimestamp, setSuggestionTimestamp] = useState<number>(0) // When current suggestion was generated

  // Agent call management (debouncing + cancellation)
  const abortControllerRef = useRef<AbortController | null>(null)
  const debounceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const maxWaitTimerRef = useRef<NodeJS.Timeout | null>(null)
  const pendingMessagesRef = useRef<TranscriptEntry[]>([])

  // Cleanup timers on unmount
  useEffect(() => {
    return () => {
      if (debounceTimerRef.current) clearTimeout(debounceTimerRef.current)
      if (maxWaitTimerRef.current) clearTimeout(maxWaitTimerRef.current)
      if (abortControllerRef.current) abortControllerRef.current.abort()
    }
  }, [])

  // Conversation stages come directly from agent (dynamically generated)
  const stages = useMemo<ConversationStage[]>(() => {
    return agentState.conversationStages || []
  }, [agentState.conversationStages])

  // Current step data - use AI agent state if available, otherwise show empty state
  const currentStepData = useMemo<CurrentStepData | null>(() => {
    // Don't show anything until we have transcript entries
    if (transcriptEntries.length === 0) {
      return null
    }

    // If agent has generated suggestions, use them
    if (agentState.nextActions && agentState.nextActions.length > 0) {
      const currentStage = stages.find(s => s.status === "current")

      return {
        stage: agentState.currentStage || currentStage?.label || "In Progress",
        description: currentStage?.description || "AI is analyzing the conversation...",
        aiSuggestion: agentState.reasoning || undefined,
        nextActions: agentState.nextActions,
        script: agentState.nextActions[0]?.description,
        tips: [], // Tips can be generated by agent if needed
      }
    }

    // If no agent data yet, return null (shows waiting state in copilot)
    return null
  }, [agentState, stages, transcriptEntries.length])

  // Handle file selection
  const handleFileSelect = useCallback((file: File) => {
    setAudioFile(file)
    setTranscriptEntries([])
    setIsPlaying(false)
    setConversationHealth(75)
    setAgentState({}) // Reset agent state
  }, [])

  // Execute agent call with accumulated messages
  const executeAgentCall = useCallback(async (messages: TranscriptEntry[]) => {
    // Cancel any in-flight request
    if (abortControllerRef.current) {
      logger.debug("Agent cancelling previous request")
      abortControllerRef.current.abort()
    }

    abortControllerRef.current = new AbortController()
    setIsAgentProcessing(true)

    try {
      // Get the last message (most recent)
      const lastMessage = messages[messages.length - 1]

      logger.debug("Agent executing with accumulated messages", {
        messageCount: messages.length
      })

      const response = await fetch("/api/agent", {
        signal: abortControllerRef.current.signal,
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          message: {
            speaker: lastMessage.speaker,
            text: lastMessage.text,
            timestamp: lastMessage.timestamp,
          },
          currentState: agentState,
        }),
      })

      if (response.ok) {
        const { state } = await response.json()
        logger.debug("Agent response received")

        // Check if we should force update (manual refresh)
        const shouldForceUpdate = forceUpdate

        // Only update if we don't already have suggestions showing
        // OR if the stage has progressed (important update)
        // OR if manual refresh was clicked
        const hasCurrentSuggestions = agentState.nextActions && agentState.nextActions.length > 0
        const stageChanged = state.currentStage !== agentState.currentStage

        if (!hasCurrentSuggestions || stageChanged || shouldForceUpdate) {
          logger.debug("Agent updating UI with new suggestions")
          setAgentState(state)
          setConversationHealth(state.healthScore || 75)
          setSuggestionTimestamp(Date.now()) // Record when suggestion was shown
          setForceUpdate(false) // Reset flag
        } else {
          logger.debug("Agent keeping current suggestions visible")
          // Update background state but don't change UI
          // This keeps context fresh without overwhelming the agent
          setAgentState(prev => ({
            ...state,
            nextActions: prev.nextActions, // Keep current actions visible
            reasoning: prev.reasoning, // Keep current reasoning
          }))
          setConversationHealth(state.healthScore || 75)
          // Don't update timestamp - keep showing age of current visible suggestion
        }
      }
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        logger.debug("Agent request cancelled (newer message arrived)")
        return
      }
      logger.error("Agent call failed", { error })
    } finally {
      setIsAgentProcessing(false)
    }
  }, [agentState])

  // Call AI Agent with debouncing + cancellation
  const callAgent = useCallback((newEntry: TranscriptEntry) => {
    // Add to pending messages
    pendingMessagesRef.current.push(newEntry)
    logger.debug("Agent message queued", {
      pendingCount: pendingMessagesRef.current.length
    })

    // Cancel existing debounce timer
    if (debounceTimerRef.current) {
      logger.debug("Agent resetting debounce timer")
      clearTimeout(debounceTimerRef.current)
    }

    // Set up max wait timer (force execution after 8s even if still receiving messages)
    if (!maxWaitTimerRef.current) {
      logger.debug("Agent starting max wait timer (8s)")
      maxWaitTimerRef.current = setTimeout(() => {
        logger.debug("Agent max wait timer triggered - forcing execution")
        if (debounceTimerRef.current) {
          clearTimeout(debounceTimerRef.current)
        }
        const messages = [...pendingMessagesRef.current]
        pendingMessagesRef.current = []
        maxWaitTimerRef.current = null
        executeAgentCall(messages)
      }, 8000) // Max 8 seconds wait
    }

    // Debounce: wait 2.5s of silence before calling agent
    debounceTimerRef.current = setTimeout(() => {
      logger.debug("Agent debounce timer triggered - executing")
      if (maxWaitTimerRef.current) {
        clearTimeout(maxWaitTimerRef.current)
        maxWaitTimerRef.current = null
      }
      const messages = [...pendingMessagesRef.current]
      pendingMessagesRef.current = []
      executeAgentCall(messages)
    }, 2500) // 2.5 second debounce
  }, [executeAgentCall])

  // Handle transcript updates from playback
  const handleTranscriptUpdate = useCallback((event: any) => {
    setCurrentTime(event.timestamp)

    // Skip empty transcriptions (silence or API returned no text)
    if (!event.text || !event.text.trim() || event.isEmpty) {
      return
    }

    setTranscriptEntries((prev) => {
      // Check if this message already exists
      const exists = prev.some(
        (entry) =>
          entry.timestamp === event.timestamp && entry.text === event.text
      )

      if (exists) {
        return prev
      }

      // Check if we should merge with the last entry from the same speaker
      const lastEntry = prev[prev.length - 1]
      const shouldMerge =
        lastEntry &&
        lastEntry.speaker === event.speaker &&
        event.timestamp - lastEntry.timestamp < 5 // Merge if within 5 seconds

      if (shouldMerge) {
        // Create merged entry
        const mergedEntry: TranscriptEntry = {
          ...lastEntry,
          text: `${lastEntry.text} ${event.text}`,
          confidence: (lastEntry.confidence! + event.confidence) / 2, // Average confidence
        }

        // DON'T call agent yet - we're still accumulating from same speaker
        // The agent will be called when speaker changes or after a pause

        // Replace last entry with merged one
        return [...prev.slice(0, -1), mergedEntry]
      }

      // Create new entry (new speaker or time gap)
      const newEntry: TranscriptEntry = {
        id: `entry-${event.chunkIndex}-${event.timestamp}`,
        timestamp: event.timestamp,
        text: event.text,
        speaker: event.speaker,
        confidence: event.confidence,
        isFinal: true,
      }

      const newEntries = [...prev, newEntry]

      // Call AI agent when speaker changes or significant gap
      // This ensures we send complete thoughts, not fragments
      if (lastEntry && lastEntry.speaker !== event.speaker) {
        // Speaker changed - send the PREVIOUS speaker's complete message
        callAgent(lastEntry)
      } else if (!lastEntry) {
        // First message - send it
        callAgent(newEntry)
      }
      // If same speaker but time gap, the debounce will handle it

      return newEntries
    })
  }, [callAgent])

  const handleActionClick = useCallback((actionId: string) => {
    logger.info("Action clicked", { actionId })

    // Clear the current suggestions to allow new ones to appear
    // This signals "I'm working on this suggestion, show me the next one when ready"
    setAgentState(prev => ({
      ...prev,
      nextActions: [],
      reasoning: "",
    }))

    // TODO: Implement action handling logic
    // This will trigger specific actions like checking availability, generating quotes, etc.
  }, [])

  const handleFeedback = useCallback((actionId: string, positive: boolean) => {
    // TODO: Track agent feedback for ML improvement
    logger.info("Feedback received", { actionId, positive })
  }, [])

  const handleRefresh = useCallback(() => {
    logger.debug("Agent manual refresh requested")

    // Set force update flag to bypass UI update logic
    setForceUpdate(true)

    // Clear current suggestions to allow new ones
    setAgentState(prev => ({
      ...prev,
      nextActions: [],
      reasoning: "",
    }))

    // Force-call agent immediately with last message (bypass debounce)
    if (transcriptEntries.length > 0) {
      const lastEntry = transcriptEntries[transcriptEntries.length - 1]

      // Cancel any pending debounce timers
      if (debounceTimerRef.current) {
        clearTimeout(debounceTimerRef.current)
        debounceTimerRef.current = null
      }
      if (maxWaitTimerRef.current) {
        clearTimeout(maxWaitTimerRef.current)
        maxWaitTimerRef.current = null
      }

      // Clear pending messages and execute immediately
      pendingMessagesRef.current = []
      executeAgentCall([lastEntry])
    }
  }, [transcriptEntries, executeAgentCall])

  return (
    <div className="h-screen bg-gray-50 flex flex-col overflow-hidden">
      {/* Header */}
      <div className="bg-white border-b flex-shrink-0">
        <div className="max-w-7xl mx-auto px-4 py-3">
          <div className="flex items-center justify-between">
            <div className="flex items-center gap-3">
              <h1 className="text-xl font-bold text-gray-900">CloseLoop</h1>
              <span className="text-sm text-gray-500">AI Call Center Copilot</span>
            </div>
            {audioFile && (
              <button
                type="button"
                onClick={() => {
                  setAudioFile(null)
                  setTranscriptEntries([])
                  setIsPlaying(false)
                  setConversationHealth(75)
                  setAgentState({})
                }}
                className="px-3 py-1.5 text-sm text-gray-600 hover:text-gray-900 border rounded-lg hover:bg-gray-50"
              >
                Upload New Audio
              </button>
            )}
          </div>
        </div>
      </div>

      <div className="flex-1 min-h-0 overflow-hidden">
        <div className="max-w-7xl mx-auto px-4 py-6 h-full flex flex-col">
        {/* Audio Upload Section */}
        {!audioFile && (
          <div className="flex-1 overflow-auto">
          <div className="max-w-2xl mx-auto">
            <div className="text-center mb-8">
              <h2 className="text-2xl font-semibold text-gray-900 mb-3">
                Upload a Call Recording
              </h2>
              <p className="text-gray-600">
                Upload an audio or video file to see real-time transcription with AI-powered conversation guidance
              </p>
            </div>
            <AudioUpload onFileSelect={handleFileSelect} />

            <div className="mt-8 bg-blue-50 border border-blue-200 rounded-lg p-6">
              <h3 className="font-semibold text-blue-900 mb-3">How it works:</h3>
              <ol className="space-y-2 text-sm text-blue-800">
                <li className="flex gap-2">
                  <span className="font-semibold">1.</span>
                  <span>Upload an audio/video file (MP3, WAV, M4A, MP4, WebM)</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">2.</span>
                  <span>Play the audio and watch real-time transcription</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">3.</span>
                  <span>See AI-powered conversation guidance based on the dialogue</span>
                </li>
                <li className="flex gap-2">
                  <span className="font-semibold">4.</span>
                  <span>Follow suggested paths to optimize the conversation outcome</span>
                </li>
              </ol>
            </div>
          </div>
          </div>
        )}

        {/* Main Demo Interface */}
        {audioFile && (
          <div className="grid lg:grid-cols-2 gap-6 flex-1 min-h-0">
            {/* Left Column: Audio Player & Transcript */}
            <div className="flex flex-col gap-6 min-h-0">
              <div className="flex-shrink-0">
                <AudioPlaybackSimulator
                  audioFile={audioFile}
                  onTranscriptUpdate={handleTranscriptUpdate}
                  onPlaybackStateChange={setIsPlaying}
                  simulateRealtime={true}
                  chunkDuration={2}
                />
              </div>

              <div className="flex-1 min-h-0">
                <LiveTranscript
                  entries={transcriptEntries}
                  isListening={isPlaying}
                  currentTime={currentTime}
                  conversationHealth={conversationHealth}
                  currentStage={agentState.currentStage || stages.find(s => s.status === "current")?.label || "In Progress"}
                  autoScroll={true}
                  showConfidence={true}
                />
              </div>
            </div>

            {/* Right Column: AI Copilot */}
            <div className="min-h-0">
              <AgentCopilot
                stages={stages}
                currentStep={currentStepData}
                conversationHealth={conversationHealth}
                isProcessing={isAgentProcessing}
                suggestionTimestamp={suggestionTimestamp}
                backgroundTasks={agentState.backgroundTasks || []}
                onActionClick={handleActionClick}
                onFeedback={handleFeedback}
                onRefresh={handleRefresh}
              />
            </div>
          </div>
        )}
        </div>
      </div>
    </div>
  )
}
